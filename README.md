# train_model


#  Adversarial Robustness Challenge 


##  Objectifs

- Entra√Æner un classifieur CNN pour reconna√Ætre des chiffres manuscrits
- G√©n√©rer des exemples adversaires avec **FGSM**
- R√©entra√Æner le mod√®le avec **adversarial training**
- √âvaluer la robustesse du mod√®le
- Visualiser les effets des attaques


##  Donn√©es utilis√©es

Dataset : [üîó Kaggle - Digit Recognizer](https://www.kaggle.com/competitions/digit-recognizer/data)

Bas√© sur **MNIST**



## Visualisation (exemples)

Le notebook compare :
- L‚Äôimage propre (avec pr√©diction correcte)
- L‚Äôimage perturb√©e (avec pr√©diction erron√©e)
- En couleur BLEUE si la pr√©diction est correcte, ROUGE sinon



##  Reproduire l‚Äôexp√©rience

###  Environnement

- Kaggle Notebook (GPU activ√©)
- TensorFlow ‚â• 2.14
- Python ‚â• 3.10

###  √âtapes

```bash
# Cloner le projet
git clone https://github.com/gatienseven7/train_model.git
cd adversarial-mnist

# Lancer le notebook sur Kaggle (ou Google Colab)


## Ce que vous apprendrez

- Comment les mod√®les de ML peuvent √™tre vuln√©rables
- Comment g√©n√©rer des attaques simples (FGSM)
- Comment rendre un mod√®le plus robuste face √† ces attaques
- Visualisation et interpr√©tation des failles

